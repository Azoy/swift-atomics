//===----------------------------------------------------------------------===//
//
// This source file is part of the Swift.org open source project
//
// Copyright (c) 2023 Apple Inc. and the Swift project authors
// Licensed under Apache License v2.0 with Runtime Library Exception
//
// See https://swift.org/LICENSE.txt for license information
// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors
//
//===----------------------------------------------------------------------===//


// #############################################################################
// #                                                                           #
// #            DO NOT EDIT THIS FILE; IT IS AUTOGENERATED.                    #
// #                                                                           #
// #############################################################################


#if !ATOMICS_NATIVE_BUILTINS
import _AtomicsShims

@_alwaysEmitIntoClient
@_transparent
internal func _atomicMemoryFence(
  ordering: AtomicUpdateOrdering
) {
  switch ordering {
    case .relaxed:
      break
  case .acquiring:
      _sa_thread_fence_acquire()
  case .releasing:
      _sa_thread_fence_release()
  case .acquiringAndReleasing:
      _sa_thread_fence_acq_rel()
  case .sequentiallyConsistent:
      _sa_thread_fence_seq_cst()
    default:
      fatalError("Unsupported ordering")
  }
}

extension UnsafeMutablePointer where Pointee == _AtomicInt8Storage {
  /// Atomically loads a word starting at this address with the specified
  /// memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicLoad(ordering: AtomicLoadOrdering) -> _AtomicInt8Storage {
    switch ordering {
    case .relaxed:
      return _sa_load_relaxed_Int8(self)
    case .acquiring:
      return _sa_load_acquire_Int8(self)
    case .sequentiallyConsistent:
      return _sa_load_seq_cst_Int8(self)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicStore(
    _ desired: _AtomicInt8Storage,
    ordering: AtomicStoreOrdering
  ) {
    switch ordering {
    case .relaxed:
      _sa_store_relaxed_Int8(self, desired)
    case .releasing:
      _sa_store_release_Int8(self, desired)
    case .sequentiallyConsistent:
      _sa_store_seq_cst_Int8(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicExchange(
    _ desired: _AtomicInt8Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt8Storage {
    switch ordering {
    case .relaxed:
      return _sa_exchange_relaxed_Int8(self, desired)
    case .acquiring:
      return _sa_exchange_acquire_Int8(self, desired)
    case .releasing:
      return _sa_exchange_release_Int8(self, desired)
    case .acquiringAndReleasing:
      return _sa_exchange_acq_rel_Int8(self, desired)
    case .sequentiallyConsistent:
      return _sa_exchange_seq_cst_Int8(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Perform an atomic compare and exchange operation with the specified memory
  /// ordering.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicInt8Storage,
    desired: _AtomicInt8Storage,
    ordering: AtomicUpdateOrdering
  ) -> (exchanged: Bool, original: _AtomicInt8Storage) {
    var expected = expected
    let exchanged: Bool
    switch ordering {
    case .relaxed:
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int8(
        self, &expected, desired)
    case .acquiring:
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int8(
        self, &expected, desired)
    case .releasing:
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int8(
        self, &expected, desired)
    case .acquiringAndReleasing:
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int8(
        self, &expected, desired)
    case .sequentiallyConsistent:
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    default:
      fatalError("Unsupported ordering")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicInt8Storage,
    desired: _AtomicInt8Storage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicInt8Storage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int8(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int8(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_strong_acquire_relaxed_Int8(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int8(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int8(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int8(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_acq_rel_relaxed_Int8(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int8(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_strong_seq_cst_relaxed_Int8(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_strong_seq_cst_acquire_Int8(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicWeakCompareExchange(
    expected: _AtomicInt8Storage,
    desired: _AtomicInt8Storage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicInt8Storage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_weak_relaxed_relaxed_Int8(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int8(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_weak_acquire_relaxed_Int8(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int8(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_release_relaxed_Int8(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int8(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_acq_rel_relaxed_Int8(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int8(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_weak_seq_cst_relaxed_Int8(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_weak_seq_cst_acquire_Int8(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int8(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic wrapping add operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&+` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingIncrement(
    by operand: _AtomicInt8Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt8Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_add_relaxed_Int8(
        self, operand)
    case .acquiring:
      return _sa_fetch_add_acquire_Int8(
        self, operand)
    case .releasing:
      return _sa_fetch_add_release_Int8(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_add_acq_rel_Int8(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_add_seq_cst_Int8(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic wrapping subtract operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&-` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingDecrement(
    by operand: _AtomicInt8Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt8Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_sub_relaxed_Int8(
        self, operand)
    case .acquiring:
      return _sa_fetch_sub_acquire_Int8(
        self, operand)
    case .releasing:
      return _sa_fetch_sub_release_Int8(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_sub_acq_rel_Int8(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_sub_seq_cst_Int8(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise AND operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseAnd(
    with operand: _AtomicInt8Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt8Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_and_relaxed_Int8(
        self, operand)
    case .acquiring:
      return _sa_fetch_and_acquire_Int8(
        self, operand)
    case .releasing:
      return _sa_fetch_and_release_Int8(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_and_acq_rel_Int8(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_and_seq_cst_Int8(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise OR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseOr(
    with operand: _AtomicInt8Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt8Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_or_relaxed_Int8(
        self, operand)
    case .acquiring:
      return _sa_fetch_or_acquire_Int8(
        self, operand)
    case .releasing:
      return _sa_fetch_or_release_Int8(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_or_acq_rel_Int8(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_or_seq_cst_Int8(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise XOR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseXor(
    with operand: _AtomicInt8Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt8Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_xor_relaxed_Int8(
        self, operand)
    case .acquiring:
      return _sa_fetch_xor_acquire_Int8(
        self, operand)
    case .releasing:
      return _sa_fetch_xor_release_Int8(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_xor_acq_rel_Int8(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_xor_seq_cst_Int8(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
}
extension UnsafeMutablePointer where Pointee == _AtomicInt16Storage {
  /// Atomically loads a word starting at this address with the specified
  /// memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicLoad(ordering: AtomicLoadOrdering) -> _AtomicInt16Storage {
    switch ordering {
    case .relaxed:
      return _sa_load_relaxed_Int16(self)
    case .acquiring:
      return _sa_load_acquire_Int16(self)
    case .sequentiallyConsistent:
      return _sa_load_seq_cst_Int16(self)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicStore(
    _ desired: _AtomicInt16Storage,
    ordering: AtomicStoreOrdering
  ) {
    switch ordering {
    case .relaxed:
      _sa_store_relaxed_Int16(self, desired)
    case .releasing:
      _sa_store_release_Int16(self, desired)
    case .sequentiallyConsistent:
      _sa_store_seq_cst_Int16(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicExchange(
    _ desired: _AtomicInt16Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt16Storage {
    switch ordering {
    case .relaxed:
      return _sa_exchange_relaxed_Int16(self, desired)
    case .acquiring:
      return _sa_exchange_acquire_Int16(self, desired)
    case .releasing:
      return _sa_exchange_release_Int16(self, desired)
    case .acquiringAndReleasing:
      return _sa_exchange_acq_rel_Int16(self, desired)
    case .sequentiallyConsistent:
      return _sa_exchange_seq_cst_Int16(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Perform an atomic compare and exchange operation with the specified memory
  /// ordering.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicInt16Storage,
    desired: _AtomicInt16Storage,
    ordering: AtomicUpdateOrdering
  ) -> (exchanged: Bool, original: _AtomicInt16Storage) {
    var expected = expected
    let exchanged: Bool
    switch ordering {
    case .relaxed:
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int16(
        self, &expected, desired)
    case .acquiring:
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int16(
        self, &expected, desired)
    case .releasing:
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int16(
        self, &expected, desired)
    case .acquiringAndReleasing:
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int16(
        self, &expected, desired)
    case .sequentiallyConsistent:
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    default:
      fatalError("Unsupported ordering")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicInt16Storage,
    desired: _AtomicInt16Storage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicInt16Storage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int16(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int16(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_strong_acquire_relaxed_Int16(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int16(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int16(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int16(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_acq_rel_relaxed_Int16(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int16(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_strong_seq_cst_relaxed_Int16(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_strong_seq_cst_acquire_Int16(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicWeakCompareExchange(
    expected: _AtomicInt16Storage,
    desired: _AtomicInt16Storage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicInt16Storage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_weak_relaxed_relaxed_Int16(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int16(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_weak_acquire_relaxed_Int16(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int16(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_release_relaxed_Int16(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int16(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_acq_rel_relaxed_Int16(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int16(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_weak_seq_cst_relaxed_Int16(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_weak_seq_cst_acquire_Int16(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int16(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic wrapping add operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&+` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingIncrement(
    by operand: _AtomicInt16Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt16Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_add_relaxed_Int16(
        self, operand)
    case .acquiring:
      return _sa_fetch_add_acquire_Int16(
        self, operand)
    case .releasing:
      return _sa_fetch_add_release_Int16(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_add_acq_rel_Int16(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_add_seq_cst_Int16(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic wrapping subtract operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&-` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingDecrement(
    by operand: _AtomicInt16Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt16Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_sub_relaxed_Int16(
        self, operand)
    case .acquiring:
      return _sa_fetch_sub_acquire_Int16(
        self, operand)
    case .releasing:
      return _sa_fetch_sub_release_Int16(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_sub_acq_rel_Int16(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_sub_seq_cst_Int16(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise AND operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseAnd(
    with operand: _AtomicInt16Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt16Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_and_relaxed_Int16(
        self, operand)
    case .acquiring:
      return _sa_fetch_and_acquire_Int16(
        self, operand)
    case .releasing:
      return _sa_fetch_and_release_Int16(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_and_acq_rel_Int16(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_and_seq_cst_Int16(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise OR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseOr(
    with operand: _AtomicInt16Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt16Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_or_relaxed_Int16(
        self, operand)
    case .acquiring:
      return _sa_fetch_or_acquire_Int16(
        self, operand)
    case .releasing:
      return _sa_fetch_or_release_Int16(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_or_acq_rel_Int16(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_or_seq_cst_Int16(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise XOR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseXor(
    with operand: _AtomicInt16Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt16Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_xor_relaxed_Int16(
        self, operand)
    case .acquiring:
      return _sa_fetch_xor_acquire_Int16(
        self, operand)
    case .releasing:
      return _sa_fetch_xor_release_Int16(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_xor_acq_rel_Int16(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_xor_seq_cst_Int16(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
}
extension UnsafeMutablePointer where Pointee == _AtomicInt32Storage {
  /// Atomically loads a word starting at this address with the specified
  /// memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicLoad(ordering: AtomicLoadOrdering) -> _AtomicInt32Storage {
    switch ordering {
    case .relaxed:
      return _sa_load_relaxed_Int32(self)
    case .acquiring:
      return _sa_load_acquire_Int32(self)
    case .sequentiallyConsistent:
      return _sa_load_seq_cst_Int32(self)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicStore(
    _ desired: _AtomicInt32Storage,
    ordering: AtomicStoreOrdering
  ) {
    switch ordering {
    case .relaxed:
      _sa_store_relaxed_Int32(self, desired)
    case .releasing:
      _sa_store_release_Int32(self, desired)
    case .sequentiallyConsistent:
      _sa_store_seq_cst_Int32(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicExchange(
    _ desired: _AtomicInt32Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt32Storage {
    switch ordering {
    case .relaxed:
      return _sa_exchange_relaxed_Int32(self, desired)
    case .acquiring:
      return _sa_exchange_acquire_Int32(self, desired)
    case .releasing:
      return _sa_exchange_release_Int32(self, desired)
    case .acquiringAndReleasing:
      return _sa_exchange_acq_rel_Int32(self, desired)
    case .sequentiallyConsistent:
      return _sa_exchange_seq_cst_Int32(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Perform an atomic compare and exchange operation with the specified memory
  /// ordering.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicInt32Storage,
    desired: _AtomicInt32Storage,
    ordering: AtomicUpdateOrdering
  ) -> (exchanged: Bool, original: _AtomicInt32Storage) {
    var expected = expected
    let exchanged: Bool
    switch ordering {
    case .relaxed:
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int32(
        self, &expected, desired)
    case .acquiring:
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int32(
        self, &expected, desired)
    case .releasing:
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int32(
        self, &expected, desired)
    case .acquiringAndReleasing:
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int32(
        self, &expected, desired)
    case .sequentiallyConsistent:
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    default:
      fatalError("Unsupported ordering")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicInt32Storage,
    desired: _AtomicInt32Storage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicInt32Storage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int32(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int32(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_strong_acquire_relaxed_Int32(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int32(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int32(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int32(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_acq_rel_relaxed_Int32(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int32(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_strong_seq_cst_relaxed_Int32(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_strong_seq_cst_acquire_Int32(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicWeakCompareExchange(
    expected: _AtomicInt32Storage,
    desired: _AtomicInt32Storage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicInt32Storage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_weak_relaxed_relaxed_Int32(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int32(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_weak_acquire_relaxed_Int32(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int32(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_release_relaxed_Int32(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int32(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_acq_rel_relaxed_Int32(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int32(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_weak_seq_cst_relaxed_Int32(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_weak_seq_cst_acquire_Int32(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int32(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic wrapping add operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&+` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingIncrement(
    by operand: _AtomicInt32Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt32Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_add_relaxed_Int32(
        self, operand)
    case .acquiring:
      return _sa_fetch_add_acquire_Int32(
        self, operand)
    case .releasing:
      return _sa_fetch_add_release_Int32(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_add_acq_rel_Int32(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_add_seq_cst_Int32(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic wrapping subtract operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&-` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingDecrement(
    by operand: _AtomicInt32Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt32Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_sub_relaxed_Int32(
        self, operand)
    case .acquiring:
      return _sa_fetch_sub_acquire_Int32(
        self, operand)
    case .releasing:
      return _sa_fetch_sub_release_Int32(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_sub_acq_rel_Int32(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_sub_seq_cst_Int32(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise AND operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseAnd(
    with operand: _AtomicInt32Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt32Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_and_relaxed_Int32(
        self, operand)
    case .acquiring:
      return _sa_fetch_and_acquire_Int32(
        self, operand)
    case .releasing:
      return _sa_fetch_and_release_Int32(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_and_acq_rel_Int32(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_and_seq_cst_Int32(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise OR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseOr(
    with operand: _AtomicInt32Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt32Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_or_relaxed_Int32(
        self, operand)
    case .acquiring:
      return _sa_fetch_or_acquire_Int32(
        self, operand)
    case .releasing:
      return _sa_fetch_or_release_Int32(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_or_acq_rel_Int32(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_or_seq_cst_Int32(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise XOR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseXor(
    with operand: _AtomicInt32Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt32Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_xor_relaxed_Int32(
        self, operand)
    case .acquiring:
      return _sa_fetch_xor_acquire_Int32(
        self, operand)
    case .releasing:
      return _sa_fetch_xor_release_Int32(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_xor_acq_rel_Int32(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_xor_seq_cst_Int32(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
}
extension UnsafeMutablePointer where Pointee == _AtomicInt64Storage {
  /// Atomically loads a word starting at this address with the specified
  /// memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicLoad(ordering: AtomicLoadOrdering) -> _AtomicInt64Storage {
    switch ordering {
    case .relaxed:
      return _sa_load_relaxed_Int64(self)
    case .acquiring:
      return _sa_load_acquire_Int64(self)
    case .sequentiallyConsistent:
      return _sa_load_seq_cst_Int64(self)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicStore(
    _ desired: _AtomicInt64Storage,
    ordering: AtomicStoreOrdering
  ) {
    switch ordering {
    case .relaxed:
      _sa_store_relaxed_Int64(self, desired)
    case .releasing:
      _sa_store_release_Int64(self, desired)
    case .sequentiallyConsistent:
      _sa_store_seq_cst_Int64(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicExchange(
    _ desired: _AtomicInt64Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt64Storage {
    switch ordering {
    case .relaxed:
      return _sa_exchange_relaxed_Int64(self, desired)
    case .acquiring:
      return _sa_exchange_acquire_Int64(self, desired)
    case .releasing:
      return _sa_exchange_release_Int64(self, desired)
    case .acquiringAndReleasing:
      return _sa_exchange_acq_rel_Int64(self, desired)
    case .sequentiallyConsistent:
      return _sa_exchange_seq_cst_Int64(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Perform an atomic compare and exchange operation with the specified memory
  /// ordering.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicInt64Storage,
    desired: _AtomicInt64Storage,
    ordering: AtomicUpdateOrdering
  ) -> (exchanged: Bool, original: _AtomicInt64Storage) {
    var expected = expected
    let exchanged: Bool
    switch ordering {
    case .relaxed:
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int64(
        self, &expected, desired)
    case .acquiring:
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int64(
        self, &expected, desired)
    case .releasing:
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int64(
        self, &expected, desired)
    case .acquiringAndReleasing:
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int64(
        self, &expected, desired)
    case .sequentiallyConsistent:
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    default:
      fatalError("Unsupported ordering")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicInt64Storage,
    desired: _AtomicInt64Storage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicInt64Storage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int64(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int64(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_strong_acquire_relaxed_Int64(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int64(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int64(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int64(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_acq_rel_relaxed_Int64(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int64(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_strong_seq_cst_relaxed_Int64(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_strong_seq_cst_acquire_Int64(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicWeakCompareExchange(
    expected: _AtomicInt64Storage,
    desired: _AtomicInt64Storage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicInt64Storage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_weak_relaxed_relaxed_Int64(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int64(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_weak_acquire_relaxed_Int64(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int64(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_release_relaxed_Int64(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int64(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_acq_rel_relaxed_Int64(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int64(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_weak_seq_cst_relaxed_Int64(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_weak_seq_cst_acquire_Int64(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int64(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic wrapping add operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&+` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingIncrement(
    by operand: _AtomicInt64Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt64Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_add_relaxed_Int64(
        self, operand)
    case .acquiring:
      return _sa_fetch_add_acquire_Int64(
        self, operand)
    case .releasing:
      return _sa_fetch_add_release_Int64(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_add_acq_rel_Int64(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_add_seq_cst_Int64(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic wrapping subtract operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&-` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingDecrement(
    by operand: _AtomicInt64Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt64Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_sub_relaxed_Int64(
        self, operand)
    case .acquiring:
      return _sa_fetch_sub_acquire_Int64(
        self, operand)
    case .releasing:
      return _sa_fetch_sub_release_Int64(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_sub_acq_rel_Int64(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_sub_seq_cst_Int64(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise AND operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseAnd(
    with operand: _AtomicInt64Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt64Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_and_relaxed_Int64(
        self, operand)
    case .acquiring:
      return _sa_fetch_and_acquire_Int64(
        self, operand)
    case .releasing:
      return _sa_fetch_and_release_Int64(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_and_acq_rel_Int64(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_and_seq_cst_Int64(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise OR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseOr(
    with operand: _AtomicInt64Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt64Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_or_relaxed_Int64(
        self, operand)
    case .acquiring:
      return _sa_fetch_or_acquire_Int64(
        self, operand)
    case .releasing:
      return _sa_fetch_or_release_Int64(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_or_acq_rel_Int64(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_or_seq_cst_Int64(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise XOR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseXor(
    with operand: _AtomicInt64Storage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicInt64Storage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_xor_relaxed_Int64(
        self, operand)
    case .acquiring:
      return _sa_fetch_xor_acquire_Int64(
        self, operand)
    case .releasing:
      return _sa_fetch_xor_release_Int64(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_xor_acq_rel_Int64(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_xor_seq_cst_Int64(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
}
extension UnsafeMutablePointer where Pointee == _AtomicIntStorage {
  /// Atomically loads a word starting at this address with the specified
  /// memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicLoad(ordering: AtomicLoadOrdering) -> _AtomicIntStorage {
    switch ordering {
    case .relaxed:
      return _sa_load_relaxed_Int(self)
    case .acquiring:
      return _sa_load_acquire_Int(self)
    case .sequentiallyConsistent:
      return _sa_load_seq_cst_Int(self)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicStore(
    _ desired: _AtomicIntStorage,
    ordering: AtomicStoreOrdering
  ) {
    switch ordering {
    case .relaxed:
      _sa_store_relaxed_Int(self, desired)
    case .releasing:
      _sa_store_release_Int(self, desired)
    case .sequentiallyConsistent:
      _sa_store_seq_cst_Int(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicExchange(
    _ desired: _AtomicIntStorage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicIntStorage {
    switch ordering {
    case .relaxed:
      return _sa_exchange_relaxed_Int(self, desired)
    case .acquiring:
      return _sa_exchange_acquire_Int(self, desired)
    case .releasing:
      return _sa_exchange_release_Int(self, desired)
    case .acquiringAndReleasing:
      return _sa_exchange_acq_rel_Int(self, desired)
    case .sequentiallyConsistent:
      return _sa_exchange_seq_cst_Int(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Perform an atomic compare and exchange operation with the specified memory
  /// ordering.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicIntStorage,
    desired: _AtomicIntStorage,
    ordering: AtomicUpdateOrdering
  ) -> (exchanged: Bool, original: _AtomicIntStorage) {
    var expected = expected
    let exchanged: Bool
    switch ordering {
    case .relaxed:
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int(
        self, &expected, desired)
    case .acquiring:
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int(
        self, &expected, desired)
    case .releasing:
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int(
        self, &expected, desired)
    case .acquiringAndReleasing:
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int(
        self, &expected, desired)
    case .sequentiallyConsistent:
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int(
        self, &expected, desired)
    default:
      fatalError("Unsupported ordering")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicIntStorage,
    desired: _AtomicIntStorage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicIntStorage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_Int(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_strong_acquire_relaxed_Int(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_Int(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_release_relaxed_Int(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_acq_rel_relaxed_Int(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_Int(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_strong_seq_cst_relaxed_Int(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_strong_seq_cst_acquire_Int(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_Int(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicWeakCompareExchange(
    expected: _AtomicIntStorage,
    desired: _AtomicIntStorage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicIntStorage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_weak_relaxed_relaxed_Int(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_weak_acquire_relaxed_Int(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_Int(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_release_relaxed_Int(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_acq_rel_relaxed_Int(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_Int(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_weak_seq_cst_relaxed_Int(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_weak_seq_cst_acquire_Int(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_Int(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic wrapping add operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&+` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingIncrement(
    by operand: _AtomicIntStorage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicIntStorage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_add_relaxed_Int(
        self, operand)
    case .acquiring:
      return _sa_fetch_add_acquire_Int(
        self, operand)
    case .releasing:
      return _sa_fetch_add_release_Int(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_add_acq_rel_Int(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_add_seq_cst_Int(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic wrapping subtract operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Note: This operation silently wraps around on overflow, like the
  /// `&-` operator does on `UInt` values.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenWrappingDecrement(
    by operand: _AtomicIntStorage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicIntStorage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_sub_relaxed_Int(
        self, operand)
    case .acquiring:
      return _sa_fetch_sub_acquire_Int(
        self, operand)
    case .releasing:
      return _sa_fetch_sub_release_Int(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_sub_acq_rel_Int(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_sub_seq_cst_Int(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise AND operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseAnd(
    with operand: _AtomicIntStorage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicIntStorage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_and_relaxed_Int(
        self, operand)
    case .acquiring:
      return _sa_fetch_and_acquire_Int(
        self, operand)
    case .releasing:
      return _sa_fetch_and_release_Int(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_and_acq_rel_Int(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_and_seq_cst_Int(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise OR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseOr(
    with operand: _AtomicIntStorage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicIntStorage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_or_relaxed_Int(
        self, operand)
    case .acquiring:
      return _sa_fetch_or_acquire_Int(
        self, operand)
    case .releasing:
      return _sa_fetch_or_release_Int(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_or_acq_rel_Int(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_or_seq_cst_Int(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
  /// Perform an atomic bitwise XOR operation and return the new value,
  /// with the specified memory ordering.
  ///
  /// - Returns: The original value before the operation.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal
  func _atomicLoadThenBitwiseXor(
    with operand: _AtomicIntStorage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicIntStorage {
    switch ordering {
    case .relaxed:
      return _sa_fetch_xor_relaxed_Int(
        self, operand)
    case .acquiring:
      return _sa_fetch_xor_acquire_Int(
        self, operand)
    case .releasing:
      return _sa_fetch_xor_release_Int(
        self, operand)
    case .acquiringAndReleasing:
      return _sa_fetch_xor_acq_rel_Int(
        self, operand)
    case .sequentiallyConsistent:
      return _sa_fetch_xor_seq_cst_Int(
        self, operand)
    default:
      preconditionFailure("Unsupported ordering")
    }
  }
}
extension UnsafeMutablePointer where Pointee == _AtomicDoubleWordStorage {
  /// Atomically loads a word starting at this address with the specified
  /// memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicLoad(ordering: AtomicLoadOrdering) -> _AtomicDoubleWordStorage {
    switch ordering {
    case .relaxed:
      return _sa_load_relaxed_DoubleWord(self)
    case .acquiring:
      return _sa_load_acquire_DoubleWord(self)
    case .sequentiallyConsistent:
      return _sa_load_seq_cst_DoubleWord(self)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  @usableFromInline
  internal func _atomicStore(
    _ desired: _AtomicDoubleWordStorage,
    ordering: AtomicStoreOrdering
  ) {
    switch ordering {
    case .relaxed:
      _sa_store_relaxed_DoubleWord(self, desired)
    case .releasing:
      _sa_store_release_DoubleWord(self, desired)
    case .sequentiallyConsistent:
      _sa_store_seq_cst_DoubleWord(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Atomically stores the specified value starting at the memory referenced by
  /// this pointer, with the specified memory ordering.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicExchange(
    _ desired: _AtomicDoubleWordStorage,
    ordering: AtomicUpdateOrdering
  ) -> _AtomicDoubleWordStorage {
    switch ordering {
    case .relaxed:
      return _sa_exchange_relaxed_DoubleWord(self, desired)
    case .acquiring:
      return _sa_exchange_acquire_DoubleWord(self, desired)
    case .releasing:
      return _sa_exchange_release_DoubleWord(self, desired)
    case .acquiringAndReleasing:
      return _sa_exchange_acq_rel_DoubleWord(self, desired)
    case .sequentiallyConsistent:
      return _sa_exchange_seq_cst_DoubleWord(self, desired)
    default:
      fatalError("Unsupported ordering")
    }
  }

  /// Perform an atomic compare and exchange operation with the specified memory
  /// ordering.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicDoubleWordStorage,
    desired: _AtomicDoubleWordStorage,
    ordering: AtomicUpdateOrdering
  ) -> (exchanged: Bool, original: _AtomicDoubleWordStorage) {
    var expected = expected
    let exchanged: Bool
    switch ordering {
    case .relaxed:
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_DoubleWord(
        self, &expected, desired)
    case .acquiring:
      exchanged = _sa_cmpxchg_strong_acquire_acquire_DoubleWord(
        self, &expected, desired)
    case .releasing:
      exchanged = _sa_cmpxchg_strong_release_relaxed_DoubleWord(
        self, &expected, desired)
    case .acquiringAndReleasing:
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_DoubleWord(
        self, &expected, desired)
    case .sequentiallyConsistent:
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    default:
      fatalError("Unsupported ordering")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicCompareExchange(
    expected: _AtomicDoubleWordStorage,
    desired: _AtomicDoubleWordStorage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicDoubleWordStorage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_strong_relaxed_relaxed_DoubleWord(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_DoubleWord(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_strong_acquire_relaxed_DoubleWord(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_strong_acquire_acquire_DoubleWord(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_release_relaxed_DoubleWord(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_DoubleWord(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_strong_acq_rel_relaxed_DoubleWord(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_strong_acq_rel_acquire_DoubleWord(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_strong_seq_cst_relaxed_DoubleWord(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_strong_seq_cst_acquire_DoubleWord(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_strong_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

  /// Perform an atomic compare and exchange operation with the specified
  /// success/failure memory orderings.
  ///
  /// This operation is equivalent to the following pseudocode:
  ///
  /// ```
  /// atomic(self, ordering, failureOrdering) { currentValue in
  ///   let original = currentValue
  ///   guard original == expected else { return (false, original) }
  ///   currentValue = desired
  ///   return (true, original)
  /// }
  /// ```
  ///
  /// The `ordering` argument specifies the memory ordering to use when the
  /// operation manages to update the current value, while `failureOrdering`
  /// will be used when the operation leaves the value intact.
  @_semantics("atomics.requires_constant_orderings")
  @_alwaysEmitIntoClient
  @_transparent // Debug performance
  public func _atomicWeakCompareExchange(
    expected: _AtomicDoubleWordStorage,
    desired: _AtomicDoubleWordStorage,
    successOrdering: AtomicUpdateOrdering,
    failureOrdering: AtomicLoadOrdering
  ) -> (exchanged: Bool, original: _AtomicDoubleWordStorage) {
    // FIXME: LLVM doesn't support arbitrary ordering combinations
    // yet, so upgrade the success ordering when necessary so that it
    // is at least as "strong" as the failure case.
    var expected = expected
    let exchanged: Bool
    switch (successOrdering, failureOrdering) {
    case (.relaxed, .relaxed):
      exchanged = _sa_cmpxchg_weak_relaxed_relaxed_DoubleWord(
        self, &expected, desired)
    case (.relaxed, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_DoubleWord(
        self, &expected, desired)
    case (.relaxed, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    case (.acquiring, .relaxed):
      exchanged = _sa_cmpxchg_weak_acquire_relaxed_DoubleWord(
        self, &expected, desired)
    case (.acquiring, .acquiring):
      exchanged = _sa_cmpxchg_weak_acquire_acquire_DoubleWord(
        self, &expected, desired)
    case (.acquiring, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    case (.releasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_release_relaxed_DoubleWord(
        self, &expected, desired)
    case (.releasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_DoubleWord(
        self, &expected, desired)
    case (.releasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    case (.acquiringAndReleasing, .relaxed):
      exchanged = _sa_cmpxchg_weak_acq_rel_relaxed_DoubleWord(
        self, &expected, desired)
    case (.acquiringAndReleasing, .acquiring):
      exchanged = _sa_cmpxchg_weak_acq_rel_acquire_DoubleWord(
        self, &expected, desired)
    case (.acquiringAndReleasing, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    case (.sequentiallyConsistent, .relaxed):
      exchanged = _sa_cmpxchg_weak_seq_cst_relaxed_DoubleWord(
        self, &expected, desired)
    case (.sequentiallyConsistent, .acquiring):
      exchanged = _sa_cmpxchg_weak_seq_cst_acquire_DoubleWord(
        self, &expected, desired)
    case (.sequentiallyConsistent, .sequentiallyConsistent):
      exchanged = _sa_cmpxchg_weak_seq_cst_seq_cst_DoubleWord(
        self, &expected, desired)
    default:
      preconditionFailure("Unsupported orderings")
    }
    return (exchanged, expected)
  }

}
#endif // ATOMICS_NATIVE_BUILTINS
